----
layout: single
title: HOUSING 데이터 머신러닝
----

```python
## 데이터 가져오기

import os
import tarfile
import urllib

# DOWNLOAD_ROOT = "https://github.com/rickiepark/handson-ml2/tree/master/"
DOWNLOAD_ROOT = "https://raw.githubusercontent.com/ageron/handson-ml2/master/"
HOUSING_PATH = os.path.join("datasets", "housing")
HOUSING_URL = DOWNLOAD_ROOT + "datasets/housing/housing.tgz"

def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):
    os.makedirs(housing_path, exist_ok=True)
    tgz_path = os.path.join(housing_path, "housing.tgz")
    urllib.request.urlretrieve(housing_url, tgz_path)
    housing_tgz = tarfile.open(tgz_path)
    housing_tgz.extractall(path=housing_path)
    housing_tgz.close()
    
fetch_housing_data()
```


```python
##csv 파일 읽기

import pandas as pd

def load_housing_data(housing_path=HOUSING_PATH):
    csv_path = os.path.join(housing_path, "housing.csv")
    return pd.read_csv(csv_path)
```


```python
## 데이터 보여주기

housing = load_housing_data()
housing.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>longitude</th>
      <th>latitude</th>
      <th>housing_median_age</th>
      <th>total_rooms</th>
      <th>total_bedrooms</th>
      <th>population</th>
      <th>households</th>
      <th>median_income</th>
      <th>median_house_value</th>
      <th>ocean_proximity</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>-122.23</td>
      <td>37.88</td>
      <td>41.0</td>
      <td>880.0</td>
      <td>129.0</td>
      <td>322.0</td>
      <td>126.0</td>
      <td>8.3252</td>
      <td>452600.0</td>
      <td>NEAR BAY</td>
    </tr>
    <tr>
      <th>1</th>
      <td>-122.22</td>
      <td>37.86</td>
      <td>21.0</td>
      <td>7099.0</td>
      <td>1106.0</td>
      <td>2401.0</td>
      <td>1138.0</td>
      <td>8.3014</td>
      <td>358500.0</td>
      <td>NEAR BAY</td>
    </tr>
    <tr>
      <th>2</th>
      <td>-122.24</td>
      <td>37.85</td>
      <td>52.0</td>
      <td>1467.0</td>
      <td>190.0</td>
      <td>496.0</td>
      <td>177.0</td>
      <td>7.2574</td>
      <td>352100.0</td>
      <td>NEAR BAY</td>
    </tr>
    <tr>
      <th>3</th>
      <td>-122.25</td>
      <td>37.85</td>
      <td>52.0</td>
      <td>1274.0</td>
      <td>235.0</td>
      <td>558.0</td>
      <td>219.0</td>
      <td>5.6431</td>
      <td>341300.0</td>
      <td>NEAR BAY</td>
    </tr>
    <tr>
      <th>4</th>
      <td>-122.25</td>
      <td>37.85</td>
      <td>52.0</td>
      <td>1627.0</td>
      <td>280.0</td>
      <td>565.0</td>
      <td>259.0</td>
      <td>3.8462</td>
      <td>342200.0</td>
      <td>NEAR BAY</td>
    </tr>
  </tbody>
</table>
</div>




```python
## 데이터 정보

housing.info()
```

    <class 'pandas.core.frame.DataFrame'>
    RangeIndex: 20640 entries, 0 to 20639
    Data columns (total 10 columns):
     #   Column              Non-Null Count  Dtype  
    ---  ------              --------------  -----  
     0   longitude           20640 non-null  float64
     1   latitude            20640 non-null  float64
     2   housing_median_age  20640 non-null  float64
     3   total_rooms         20640 non-null  float64
     4   total_bedrooms      20433 non-null  float64
     5   population          20640 non-null  float64
     6   households          20640 non-null  float64
     7   median_income       20640 non-null  float64
     8   median_house_value  20640 non-null  float64
     9   ocean_proximity     20640 non-null  object 
    dtypes: float64(9), object(1)
    memory usage: 1.6+ MB
    


```python
## 범주형 데이터 보여주기

housing["ocean_proximity"].value_counts()
```




    <1H OCEAN     9136
    INLAND        6551
    NEAR OCEAN    2658
    NEAR BAY      2290
    ISLAND           5
    Name: ocean_proximity, dtype: int64




```python
## 수치형 데이터 보여주기

%matplotlib inline
import matplotlib.pyplot as plt
housing.hist(bins=50, figsize=(20,15))
plt.show()
```


    
![png](output_5_0.png)
    


## 무작위 샘플링 방식


```python
## 훈련세트, 테스트세트 나누기

import numpy as np

def split_train_test(data, test_ratio):
    shuffled_indices = np.random.permutation(len(data))
    test_set_size = int(len(data) * test_ratio)
    test_indices = shuffled_indices[:test_set_size]
    train_indices = shuffled_indices[test_set_size:]
    return data.iloc[train_indices], data.iloc[test_indices]

train_set, test_set = split_train_test(housing, 0.2)
```


```python
print(len(train_set))
print(len(test_set))
```

    16512
    4128
    


```python
## 데이터셋 업데이트 후에도 안정적인 훈련/테스트 분할을 위한 해결책 -> 샘플의 식별자 사용하여 테스트 세트로 보낼지 결정
## 조건: 새 데이터는 데이터 셋의 끝에 추가, 어떤 행도 삭제되지 않아야 한다

from zlib import crc32

def test_set_check(identifier, test_ratio):
    return crc32(np.int64(identifier)) & 0xffffffff < test_ratio * 2**32

def split_train_test_by_id(data, test_ratio, id_column):
    ids = data[id_column]
    in_test_set = ids.apply(lambda id_:test_set_check(id_,test_ratio))
    return data.loc[~in_test_set], data.loc[in_test_set]
```


```python
## 행의 인덱스를 ID로 사용

housing_with_id = housing.reset_index()

train_set, test_set = split_train_test_by_id(housing_with_id, 0.2, "index")
```


```python
## 고유 식별자 생성
## 사용된 데이터(위, 경도)가 몇백 년 후까지 안정적이라고 보장

housing_with_id["id"] = housing["longitude"] * 1000 + housing["latitude"]
train_set, test_set = split_train_test_by_id(housing_with_id, 0.2, "id")
```


```python
## 사이킷런 함수 사용

from sklearn.model_selection import train_test_split

train_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)
```

## 계층적 샘플링


```python
## 계층 분석
## 계층별로 데이터셋에 충분한 샘플 수가 있어야 한다 -> 너무 많은 계층으로 나누면 안 된다

housing["income_cat"] = pd.cut(housing["median_income"], bins=[0., 1.5, 3.0, 4.5, 6., np.inf],
                              labels=[1, 2, 3, 4, 5])

housing["income_cat"].hist()
```




    <AxesSubplot:>




    
![png](output_14_1.png)
    



```python
## 계층적 샘플링 구현

from sklearn.model_selection import StratifiedShuffleSplit

split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)
for train_index, test_index in split.split(housing, housing["income_cat"]):
    start_train_set = housing.loc[train_index]
    start_test_set = housing.loc[test_index]
```


```python
## 계층적 샘플링 확인

start_test_set["income_cat"].value_counts()/len(start_test_set)
```




    3    0.350533
    2    0.318798
    4    0.176357
    5    0.114341
    1    0.039971
    Name: income_cat, dtype: float64




```python
## income_cat 특성 삭제 후, 데이터 원래 상태로 복구

for set_ in (start_train_set, start_test_set):
    set_.drop("income_cat", axis=1, inplace=True)
```

# 데이터 이해를 위한 탐색과 시각화


```python
## 훈련 세트를 손상시키지 않기 위해 복사본을 만들어 사용

housing = start_train_set.copy()
```


```python
housing.plot(kind="scatter", x="longitude", y="latitude")
```




    <AxesSubplot:xlabel='longitude', ylabel='latitude'>




    
![png](output_20_1.png)
    



```python
## 밀집 영역 파악
housing.plot(kind="scatter", x="longitude", y="latitude", alpha=0.1)
```




    <AxesSubplot:xlabel='longitude', ylabel='latitude'>




    
![png](output_21_1.png)
    



```python
## 여러 정보 시각화 -> 주택 가격은 지역과 인구 밀도에 관련이 매우 크다는 것을 발견

housing.plot(kind="scatter", x="longitude", y="latitude", alpha=0.4,
             ## s = 마커의 크기를 결정하는 파라미터 -> 구역 인구
             s=housing["population"]/100, label="population", figsize=(10,7),
             ## c = 마커의 색상을 결정하는 파라미터 -> 중간 주택 가격
             c="median_house_value", cmap=plt.get_cmap("jet"), colorbar=True,
             sharex=False)
```




    <AxesSubplot:xlabel='longitude', ylabel='latitude'>




    
![png](output_22_1.png)
    



```python
## 상관관계 조사 -> median_income과 양의 상관관계, latitude와 약한 음의 상관관계

corr_matrix = housing.corr()
corr_matrix["median_house_value"].sort_values(ascending=False)
```




    median_house_value    1.000000
    median_income         0.687151
    total_rooms           0.135140
    housing_median_age    0.114146
    households            0.064590
    total_bedrooms        0.047781
    population           -0.026882
    longitude            -0.047466
    latitude             -0.142673
    Name: median_house_value, dtype: float64




```python
from pandas.plotting import scatter_matrix

attributes = ["median_house_value", "median_income", "total_rooms",
             "housing_median_age"]
scatter_matrix(housing[attributes], figsize=(12,8))
```




    array([[<AxesSubplot:xlabel='median_house_value', ylabel='median_house_value'>,
            <AxesSubplot:xlabel='median_income', ylabel='median_house_value'>,
            <AxesSubplot:xlabel='total_rooms', ylabel='median_house_value'>,
            <AxesSubplot:xlabel='housing_median_age', ylabel='median_house_value'>],
           [<AxesSubplot:xlabel='median_house_value', ylabel='median_income'>,
            <AxesSubplot:xlabel='median_income', ylabel='median_income'>,
            <AxesSubplot:xlabel='total_rooms', ylabel='median_income'>,
            <AxesSubplot:xlabel='housing_median_age', ylabel='median_income'>],
           [<AxesSubplot:xlabel='median_house_value', ylabel='total_rooms'>,
            <AxesSubplot:xlabel='median_income', ylabel='total_rooms'>,
            <AxesSubplot:xlabel='total_rooms', ylabel='total_rooms'>,
            <AxesSubplot:xlabel='housing_median_age', ylabel='total_rooms'>],
           [<AxesSubplot:xlabel='median_house_value', ylabel='housing_median_age'>,
            <AxesSubplot:xlabel='median_income', ylabel='housing_median_age'>,
            <AxesSubplot:xlabel='total_rooms', ylabel='housing_median_age'>,
            <AxesSubplot:xlabel='housing_median_age', ylabel='housing_median_age'>]],
          dtype=object)




    
![png](output_24_1.png)
    



```python
## 중간 주택 가격 - 중간 소득 산점도 확대 -> 직선에 가까운 형태의 데이터 제거 -> 이상한 형태를 학습하지 않도록

housing.plot(kind="scatter", x="median_income", y="median_house_value", alpha=0.1)
```




    <AxesSubplot:xlabel='median_income', ylabel='median_house_value'>




    
![png](output_25_1.png)
    



```python
## 특성 조합 실험
housing["rooms_per_household"] = housing["total_rooms"]/housing["households"]
housing["bedrooms_per_room"] = housing["total_bedrooms"]/housing["total_rooms"]
housing["population_per_household"]=housing["population"]/housing["households"]


## 결과적으로, bedrooms_per_room 특성이 상관관계가 높게 나옴
corr_matrix = housing.corr()
corr_matrix["median_house_value"].sort_values(ascending=False)
```




    median_house_value          1.000000
    median_income               0.687151
    rooms_per_household         0.146255
    total_rooms                 0.135140
    housing_median_age          0.114146
    households                  0.064590
    total_bedrooms              0.047781
    population_per_household   -0.021991
    population                 -0.026882
    longitude                  -0.047466
    latitude                   -0.142673
    bedrooms_per_room          -0.259952
    Name: median_house_value, dtype: float64



# 데이터 준비


```python
## 독립변수와 종속변수 분리

housing = start_train_set.drop("median_house_value", axis=1)
housing_labels = start_train_set["median_house_value"].copy()
```


```python
## 데이터 정제

# 옵션 1 - 해당구역 제거
housing.dropna(subset=["total_bedrooms"])

# 옵션 2 - 전체특성 삭제
housing.drop("total_bedrooms", axis=1)

# 옵션 3 - 중간값 채우기
median = housing["total_bedrooms"].median()
housing["total_bedrooms"].fillna(median, inplace=True)
```


```python
## 사이킷런 함수를 이용해 훈련데이터에 적용

from sklearn.impute import SimpleImputer

imputer = SimpleImputer(strategy="median")

housing_num = housing.drop("ocean_proximity", axis=1)
imputer.fit(housing_num)
```




    SimpleImputer(strategy='median')




```python
## 비교

print(imputer.statistics_)
print(housing_num.median().values)
```

    [-118.51      34.26      29.      2119.       433.      1164.
      408.         3.54155]
    [-118.51      34.26      29.      2119.       433.      1164.
      408.         3.54155]
    


```python
## 학습된 imputer 객체를 사용해 훈련세트에서 누락된 값을 학습한 중간값으로 변형

X = imputer.transform(housing_num)
housing_tr = pd.DataFrame(X, columns=housing_num.columns,
                         index=housing_num.index)
```

### 텍스트와 범주형 특성 처리


```python
## 범주형 데이터 확인

housing_cat = housing[["ocean_proximity"]]
housing_cat.head(10)
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>ocean_proximity</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>12655</th>
      <td>INLAND</td>
    </tr>
    <tr>
      <th>15502</th>
      <td>NEAR OCEAN</td>
    </tr>
    <tr>
      <th>2908</th>
      <td>INLAND</td>
    </tr>
    <tr>
      <th>14053</th>
      <td>NEAR OCEAN</td>
    </tr>
    <tr>
      <th>20496</th>
      <td>&lt;1H OCEAN</td>
    </tr>
    <tr>
      <th>1481</th>
      <td>NEAR BAY</td>
    </tr>
    <tr>
      <th>18125</th>
      <td>&lt;1H OCEAN</td>
    </tr>
    <tr>
      <th>5830</th>
      <td>&lt;1H OCEAN</td>
    </tr>
    <tr>
      <th>17989</th>
      <td>&lt;1H OCEAN</td>
    </tr>
    <tr>
      <th>4861</th>
      <td>&lt;1H OCEAN</td>
    </tr>
  </tbody>
</table>
</div>




```python
## 카테고리 텍스트를 숫자로 변환
## 이 방법은 숫자의 차이가 유사도에 영향을 미침

from sklearn.preprocessing import OrdinalEncoder

ordinal_encoder = OrdinalEncoder()
housing_cat_encoded = ordinal_encoder.fit_transform(housing_cat)
housing_cat_encoded[:10]
```




    array([[1.],
           [4.],
           [1.],
           [4.],
           [0.],
           [3.],
           [0.],
           [0.],
           [0.],
           [0.]])




```python
## 원-핫 인코딩 -> 희소행렬

from sklearn.preprocessing import OneHotEncoder
cat_encoder = OneHotEncoder()
housing_cat_1hot = cat_encoder.fit_transform(housing_cat)
housing_cat_1hot.toarray()
```




    array([[0., 1., 0., 0., 0.],
           [0., 0., 0., 0., 1.],
           [0., 1., 0., 0., 0.],
           ...,
           [1., 0., 0., 0., 0.],
           [1., 0., 0., 0., 0.],
           [0., 1., 0., 0., 0.]])




```python
## 카테고리 리스트 확인

cat_encoder.categories_
```




    [array(['<1H OCEAN', 'INLAND', 'ISLAND', 'NEAR BAY', 'NEAR OCEAN'],
           dtype=object)]



### 나만의 변환기


```python
## 조합 특성을 추가하는 간단한 변환기

from sklearn.base import BaseEstimator, TransformerMixin

rooms_ix, bedrooms_ix, population_ix, households_ix = 3, 4, 5, 6

class CombineAttributesAdder(BaseEstimator, TransformerMixin):
    def __init__(self, add_bedrooms_per_room = True):
        self.add_bedrooms_per_room = add_bedrooms_per_room
    def fit(self, x, y=None):
        return self
    def transform(self, X):
        rooms_per_household = X[:, rooms_ix] / X[:, households_ix]
        population_per_household = X[:, population_ix] / X[:, households_ix]
        if self.add_bedrooms_per_room:
            bedrooms_per_room = X[:, bedrooms_ix] / X[:, rooms_ix]
            return np.c_[X, rooms_per_household, population_per_household,
                        bedrooms_per_room]
        else:
            return np.c_[X, rooms_per_household, population_per_household]
            
attr_adder = CombineAttributesAdder(add_bedrooms_per_room=False)
housing_extra_attribs = attr_adder.transform(housing.values)
```


```python
## 변환 파이프라인

from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler

num_pipeline = Pipeline([
    ('imputer', SimpleImputer(strategy="median")), # 전처리 - 중간값 채우기
    ('attribs_adder', CombineAttributesAdder()), # 조합 특성을 추가
    ('std_scaler', StandardScaler()) # 스케일링 - 표준화
])

housing_num_tr = num_pipeline.fit_transform(housing_num)
```


```python
## 각 열마다 적절한 변환 적용

from sklearn.compose import ColumnTransformer

num_attribs = list(housing_num)
cat_attribs = ["ocean_proximity"]

full_pipeline = ColumnTransformer([
    ("num", num_pipeline, num_attribs),
    ("cat", OneHotEncoder(), cat_attribs),
])

housing_prepared = full_pipeline.fit_transform(housing)
```

# 모델 선택과 훈련


```python
## 훈련 세트에서 모델 훈련

from sklearn.linear_model import LinearRegression

lin_reg = LinearRegression()
lin_reg.fit(housing_prepared, housing_labels)
```




    LinearRegression()




```python
some_data = housing.iloc[:5]
some_labels = housing_labels.iloc[:5]

some_data_prepared = full_pipeline.transform(some_data)
print("예측:", lin_reg.predict(some_data_prepared))
print("레이블:", list(some_labels))
```

    예측: [ 85657.90192014 305492.60737488 152056.46122456 186095.70946094
     244550.67966089]
    레이블: [72100.0, 279600.0, 82700.0, 112500.0, 238300.0]
    


```python
from sklearn.metrics import mean_squared_error

housing_predictions = lin_reg.predict(housing_prepared)
lin_mse = mean_squared_error(housing_labels, housing_predictions)
lin_rmse = np.sqrt(lin_mse)
lin_rmse
```




    68627.87390018745



### 실험결과
중간 주택 가격이 120,000에서 265,000 사이인데, 예측 오차가 68,627인 것은 만족스럽지 못한 결과 -> 과소적합
특성들이 좋지 못하거나 모델이 충분히 강력하지 못함 -> 좋은 특성 주입, 강력한 모델, 모델의 규제 감소


```python
## 강력한 모델 사용

from sklearn.tree import DecisionTreeRegressor

tree_reg = DecisionTreeRegressor()
tree_reg.fit(housing_prepared, housing_labels)
```




    DecisionTreeRegressor()




```python
housing_predictions = tree_reg.predict(housing_prepared)
tree_mse = mean_squared_error(housing_labels, housing_predictions)
tree_rmse = np.sqrt(tree_mse)
tree_rmse
```




    0.0



### 실험결과
너무 심하게 과대적합

## 교차검증 평가
#### 확신이 드는 모델이 론칭할 준비가 되기 전까지 테스트 세트는 사용하지 않는다.
#### 따라서, 훈련 세트의 일부분으로 훈련을 하고 다른 일부분은 모델 검증에 사용한다.


```python
## k-겹 교차 검증
## 훈련세트를 폴드라 불리는 10개 서브셋으로 무작위 분할 -> 매번 다른 폴드로 평가

from sklearn.model_selection import cross_val_score
tree_scores = cross_val_score(tree_reg, housing_prepared, housing_labels,
                        scoring="neg_mean_squared_error", cv=10)
tree_rmse_scores = np.sqrt(-tree_scores)
```


```python
def display_scores(scores):
    print("점수:", scores)
    print("평균:", scores.mean())
    print("표준편차:", scores.std())
    
display_scores(tree_rmse_scores)
```

    점수: [73359.67696741 70191.97713354 69765.03144241 70326.43717176
     71199.7220885  78124.47216846 71657.84966058 71597.85487915
     68142.38841224 71594.72826769]
    평균: 71596.01381917289
    표준편차: 2544.7066778265607
    


```python
## 선형모델 교차검증
lin_scores = cross_val_score(lin_reg, housing_prepared, housing_labels,
                        scoring="neg_mean_squared_error", cv=10)
lin_rmse_scores = np.sqrt(-lin_scores)

display_scores(lin_rmse_scores)
```

    점수: [71762.76364394 64114.99166359 67771.17124356 68635.19072082
     66846.14089488 72528.03725385 73997.08050233 68802.33629334
     66443.28836884 70139.79923956]
    평균: 69104.07998247063
    표준편차: 2880.328209818065
    

#### 결과적으로, 교차검증에 대한 결과는 결정 트리 모델이 과대적합되어 선형회귀모델보다 성능이 나쁩니다.


```python
## 앙상블 모델

from sklearn.ensemble import RandomForestRegressor

forest_reg = RandomForestRegressor()
forest_reg.fit(housing_prepared, housing_labels)

housing_predictions = forest_reg.predict(housing_prepared)
forest_mse = mean_squared_error(housing_labels, housing_predictions)
forest_rmse = np.sqrt(forest_mse)
forest_rmse
```




    18700.683326049362




```python
forest_scores = cross_val_score(forest_reg, housing_prepared, housing_labels,
                        scoring="neg_mean_squared_error", cv=10)
forest_rmse_scores = np.sqrt(-forest_scores)

display_scores(forest_rmse_scores)
```

    점수: [51131.15052354 49009.20307545 47046.75985198 52383.27561456
     47231.23880829 51630.21093467 52008.11183231 49834.67456101
     48452.98967741 53879.0135659 ]
    평균: 50260.66284451151
    표준편차: 2186.4770279397126
    

### 모델 세부 튜닝


```python
## 그리드 탐색

from sklearn.model_selection import GridSearchCV

param_grid = [
    {'n_estimators' : [3, 10, 30], 'max_features' : [2, 4, 6, 8]},
    {'bootstrap' : [False], 'n_estimators' : [3, 10], 'max_features' : [2, 3, 4]} 
]

forest_reg = RandomForestRegressor()

grid_search = GridSearchCV(forest_reg, param_grid, cv=5,
                           scoring='neg_mean_squared_error',
                           return_train_score = True)

grid_search.fit(housing_prepared, housing_labels)
```




    GridSearchCV(cv=5, estimator=RandomForestRegressor(),
                 param_grid=[{'max_features': [2, 4, 6, 8],
                              'n_estimators': [3, 10, 30]},
                             {'bootstrap': [False], 'max_features': [2, 3, 4],
                              'n_estimators': [3, 10]}],
                 return_train_score=True, scoring='neg_mean_squared_error')




```python
## 최적의 조합

grid_search.best_params_
```




    {'max_features': 4, 'n_estimators': 30}




```python
## 최상의 모델과 오차분석 -> 덜 중요한 특성 제외
## 예를 들어, ocean_proximity 카테고리 중 하나만 실제로 유용하므로 다른 카테고리 제외

feature_importances = grid_search.best_estimator_.feature_importances_

extra_attribs = ["rooms_per_hhold", "pop_per_hold", "bedrooms_per_room"]
cat_encoder = full_pipeline.named_transformers_["cat"]
cat_one_hot_attribs = list(cat_encoder.categories_[0])
attributes = num_attribs + extra_attribs + cat_one_hot_attribs
sorted(zip(feature_importances, attributes), reverse=True)
```




    [(0.2712019281445172, 'median_income'),
     (0.14195365358284345, 'INLAND'),
     (0.10518361170356377, 'bedrooms_per_room'),
     (0.10310019337325313, 'pop_per_hold'),
     (0.07783950702605429, 'latitude'),
     (0.07230510252651251, 'longitude'),
     (0.06306755923640633, 'rooms_per_hhold'),
     (0.04390817882927251, 'housing_median_age'),
     (0.026398441387608617, 'total_rooms'),
     (0.025976114169428563, 'population'),
     (0.022387176912629047, 'total_bedrooms'),
     (0.020549380611298743, 'households'),
     (0.014677836148627521, '<1H OCEAN'),
     (0.006419727678776659, 'NEAR OCEAN'),
     (0.004995052862494189, 'NEAR BAY'),
     (3.653580671330831e-05, 'ISLAND')]



## 테스트 세트로 시스템 평가


```python
## 최종 모델 평가
## fit_transform() -> transform()

final_model = grid_search.best_estimator_

X_test = start_test_set.drop("median_house_value", axis=1)
y_test = start_test_set["median_house_value"].copy()

X_test_prepared = full_pipeline.transform(X_test)

final_predictions = final_model.predict(X_test_prepared)

final_mse = mean_squared_error(y_test, final_predictions)
final_rmse = np.sqrt(final_mse)
print(final_rmse)
```

    48769.6564644412
    


```python
## 신뢰구간

from scipy import stats

confidence = 0.95
squared_errors = (final_predictions - y_test) ** 2
np.sqrt(stats.t.interval(confidence, len(squared_errors) - 1,
                        loc=squared_errors.mean(),
                        scale=stats.sem(squared_errors)))
```




    array([46843.5199486 , 50622.55843143])


